{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a6446c8",
   "metadata": {},
   "source": [
    "# üèè IPL Score Prediction - Model Training\n",
    "\n",
    "This notebook trains and evaluates deep learning models for IPL score prediction.\n",
    "\n",
    "**Author:** IPL Score Prediction Team  \n",
    "**Date:** 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc57594",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a927240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\vinod\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MultiHeadAttention' from 'tensorflow.keras.layers' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13368\\360386549.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_preprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIPLDataLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIPLDataCleaner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataSplitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfeature_engineering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFeatureEngineer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m from model_architectures import (\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mBaselineModels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDNNModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRUModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mTransformerModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHybridModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelFactory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\IPL\\IPL_Score_Prediction\\src\\model_architectures.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m from tensorflow.keras.layers import (\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mMultiHeadAttention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MultiHeadAttention' from 'tensorflow.keras.layers' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from data_preprocessing import IPLDataLoader, IPLDataCleaner, DataSplitter\n",
    "from feature_engineering import FeatureEngineer\n",
    "from model_architectures import (\n",
    "    BaselineModels, DNNModel, LSTMModel, GRUModel, \n",
    "    TransformerModel, HybridModel, ModelFactory\n",
    ")\n",
    "from train import TrainingConfig, ModelTrainer, TrainingVisualizer\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(\"\\n‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdab4f",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f980b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = IPLDataLoader(data_path='../data/')\n",
    "ball_df = loader.load_ball_by_ball_data()\n",
    "\n",
    "# Clean data\n",
    "cleaner = IPLDataCleaner()\n",
    "ball_df = cleaner.clean_data(ball_df)\n",
    "\n",
    "print(f\"‚úÖ Data loaded: {len(ball_df):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "feature_engineer = FeatureEngineer()\n",
    "df_features = feature_engineer.fit_transform(ball_df)\n",
    "\n",
    "print(f\"‚úÖ Feature engineering complete: {len(df_features.columns)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = feature_engineer.get_feature_columns()\n",
    "\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features['total_runs'].copy()\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"\\nüìä Dataset Shape:\")\n",
    "print(f\"   X: {X.shape}\")\n",
    "print(f\"   y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f8d82",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e15aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Further split training into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"   Training: {len(X_train):,} samples\")\n",
    "print(f\"   Validation: {len(X_val):,} samples\")\n",
    "print(f\"   Test: {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ebda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "\n",
    "print(\"‚úÖ Data normalized and scaler saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50599a5",
   "metadata": {},
   "source": [
    "## 4. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize baseline models\n",
    "baseline = BaselineModels()\n",
    "models = baseline.get_models()\n",
    "\n",
    "# Train and evaluate baseline models\n",
    "baseline_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    baseline_results[name] = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    print(f\"   MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display baseline results\n",
    "baseline_df = pd.DataFrame(baseline_results).T\n",
    "baseline_df = baseline_df.round(4)\n",
    "print(\"\\nüìä Baseline Model Results:\")\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bae2c1",
   "metadata": {},
   "source": [
    "## 5. Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = TrainingConfig(\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "    early_stopping_patience=15,\n",
    "    reduce_lr_patience=5\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config.early_stopping_patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=config.reduce_lr_patience,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/dnn_best.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Training configuration set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DNN model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "\n",
    "dnn_builder = DNNModel(\n",
    "    input_dim=input_dim,\n",
    "    hidden_units=[256, 128, 64, 32],\n",
    "    dropout_rate=0.3,\n",
    "    l2_reg=0.001\n",
    ")\n",
    "\n",
    "dnn_model = dnn_builder.build()\n",
    "\n",
    "# Compile model\n",
    "dnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=config.learning_rate),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nüìä DNN Model Architecture:\")\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd04d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DNN\n",
    "print(\"\\nüîÑ Training DNN Model...\")\n",
    "\n",
    "dnn_history = dnn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=config.epochs,\n",
    "    batch_size=config.batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "visualizer = TrainingVisualizer()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(dnn_history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(dnn_history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('DNN Training History - Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE curve\n",
    "axes[1].plot(dnn_history.history['mae'], label='Training MAE')\n",
    "axes[1].plot(dnn_history.history['val_mae'], label='Validation MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('DNN Training History - MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DNN\n",
    "dnn_pred = dnn_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "\n",
    "dnn_mae = mean_absolute_error(y_test, dnn_pred)\n",
    "dnn_rmse = np.sqrt(mean_squared_error(y_test, dnn_pred))\n",
    "dnn_r2 = r2_score(y_test, dnn_pred)\n",
    "\n",
    "print(f\"\\nüìä DNN Model Results:\")\n",
    "print(f\"   MAE: {dnn_mae:.2f}\")\n",
    "print(f\"   RMSE: {dnn_rmse:.2f}\")\n",
    "print(f\"   R¬≤: {dnn_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf674c3",
   "metadata": {},
   "source": [
    "## 6. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b39a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequence data for LSTM\n",
    "# Reshape data for LSTM: (samples, timesteps, features)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_val_lstm = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "print(f\"LSTM input shape: {X_train_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7561901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "lstm_builder = LSTMModel(\n",
    "    input_shape=(1, input_dim),\n",
    "    lstm_units=[128, 64],\n",
    "    dropout_rate=0.3,\n",
    "    use_attention=True\n",
    ")\n",
    "\n",
    "lstm_model = lstm_builder.build()\n",
    "\n",
    "# Compile model\n",
    "lstm_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=config.learning_rate),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nüìä LSTM Model Architecture:\")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422cb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM callbacks\n",
    "lstm_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config.early_stopping_patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=config.reduce_lr_patience,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/lstm_best.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train LSTM\n",
    "print(\"\\nüîÑ Training LSTM Model...\")\n",
    "\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    validation_data=(X_val_lstm, y_val),\n",
    "    epochs=config.epochs,\n",
    "    batch_size=config.batch_size,\n",
    "    callbacks=lstm_callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(lstm_history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(lstm_history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('LSTM Training History - Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(lstm_history.history['mae'], label='Training MAE')\n",
    "axes[1].plot(lstm_history.history['val_mae'], label='Validation MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('LSTM Training History - MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814812c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM\n",
    "lstm_pred = lstm_model.predict(X_test_lstm, verbose=0).flatten()\n",
    "\n",
    "lstm_mae = mean_absolute_error(y_test, lstm_pred)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_pred))\n",
    "lstm_r2 = r2_score(y_test, lstm_pred)\n",
    "\n",
    "print(f\"\\nüìä LSTM Model Results:\")\n",
    "print(f\"   MAE: {lstm_mae:.2f}\")\n",
    "print(f\"   RMSE: {lstm_rmse:.2f}\")\n",
    "print(f\"   R¬≤: {lstm_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9894a",
   "metadata": {},
   "source": [
    "## 7. GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GRU model\n",
    "gru_builder = GRUModel(\n",
    "    input_shape=(1, input_dim),\n",
    "    gru_units=[128, 64],\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "gru_model = gru_builder.build()\n",
    "\n",
    "# Compile model\n",
    "gru_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=config.learning_rate),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"\\nüìä GRU Model Architecture:\")\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU callbacks\n",
    "gru_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config.early_stopping_patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=config.reduce_lr_patience,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/gru_best.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train GRU\n",
    "print(\"\\nüîÑ Training GRU Model...\")\n",
    "\n",
    "gru_history = gru_model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    validation_data=(X_val_lstm, y_val),\n",
    "    epochs=config.epochs,\n",
    "    batch_size=config.batch_size,\n",
    "    callbacks=gru_callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1036fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRU\n",
    "gru_pred = gru_model.predict(X_test_lstm, verbose=0).flatten()\n",
    "\n",
    "gru_mae = mean_absolute_error(y_test, gru_pred)\n",
    "gru_rmse = np.sqrt(mean_squared_error(y_test, gru_pred))\n",
    "gru_r2 = r2_score(y_test, gru_pred)\n",
    "\n",
    "print(f\"\\nüìä GRU Model Results:\")\n",
    "print(f\"   MAE: {gru_mae:.2f}\")\n",
    "print(f\"   RMSE: {gru_rmse:.2f}\")\n",
    "print(f\"   R¬≤: {gru_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944946f",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784cf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "dl_results = {\n",
    "    'DNN': {'MAE': dnn_mae, 'RMSE': dnn_rmse, 'R2': dnn_r2},\n",
    "    'LSTM': {'MAE': lstm_mae, 'RMSE': lstm_rmse, 'R2': lstm_r2},\n",
    "    'GRU': {'MAE': gru_mae, 'RMSE': gru_rmse, 'R2': gru_r2}\n",
    "}\n",
    "\n",
    "# Combine with baseline\n",
    "all_results = {**baseline_results, **dl_results}\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df = results_df.round(4)\n",
    "results_df = results_df.sort_values('MAE')\n",
    "\n",
    "print(\"\\nüìä All Model Results (Sorted by MAE):\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# MAE comparison\n",
    "results_df['MAE'].plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].set_title('Model Comparison - MAE (Lower is Better)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE comparison\n",
    "results_df['RMSE'].plot(kind='bar', ax=axes[1], color='coral', edgecolor='black')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Model Comparison - RMSE (Lower is Better)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R¬≤ comparison\n",
    "results_df['R2'].plot(kind='bar', ax=axes[2], color='green', edgecolor='black')\n",
    "axes[2].set_ylabel('R¬≤')\n",
    "axes[2].set_title('Model Comparison - R¬≤ (Higher is Better)')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d373c39",
   "metadata": {},
   "source": [
    "## 9. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model for analysis\n",
    "best_model_name = results_df['MAE'].idxmin()\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "\n",
    "# Get predictions from best model\n",
    "if best_model_name == 'DNN':\n",
    "    best_pred = dnn_pred\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_pred = lstm_pred\n",
    "elif best_model_name == 'GRU':\n",
    "    best_pred = gru_pred\n",
    "else:\n",
    "    # Use best baseline\n",
    "    best_pred = models[best_model_name].predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8413a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, best_pred, alpha=0.5, edgecolors='black', linewidth=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Score')\n",
    "axes[0].set_ylabel('Predicted Score')\n",
    "axes[0].set_title(f'{best_model_name}: Actual vs Predicted')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "residuals = y_test - best_pred\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual Distribution')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis by score range\n",
    "score_ranges = [(0, 50), (50, 100), (100, 150), (150, 200), (200, 300)]\n",
    "\n",
    "range_analysis = []\n",
    "for low, high in score_ranges:\n",
    "    mask = (y_test >= low) & (y_test < high)\n",
    "    if mask.sum() > 0:\n",
    "        range_mae = mean_absolute_error(y_test[mask], best_pred[mask])\n",
    "        range_analysis.append({\n",
    "            'Range': f'{low}-{high}',\n",
    "            'Samples': mask.sum(),\n",
    "            'MAE': range_mae\n",
    "        })\n",
    "\n",
    "range_df = pd.DataFrame(range_analysis)\n",
    "print(\"\\nüìä Error Analysis by Score Range:\")\n",
    "range_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00234f90",
   "metadata": {},
   "source": [
    "## 10. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "if best_model_name == 'DNN':\n",
    "    dnn_model.save('../models/best_model.keras')\n",
    "    model_type = 'keras'\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save('../models/best_model.keras')\n",
    "    model_type = 'keras'\n",
    "elif best_model_name == 'GRU':\n",
    "    gru_model.save('../models/best_model.keras')\n",
    "    model_type = 'keras'\n",
    "else:\n",
    "    joblib.dump(models[best_model_name], '../models/best_model.pkl')\n",
    "    model_type = 'sklearn'\n",
    "\n",
    "# Save feature engineer\n",
    "joblib.dump(feature_engineer, '../models/feature_engineer.pkl')\n",
    "\n",
    "# Save model info\n",
    "model_info = {\n",
    "    'name': best_model_name,\n",
    "    'type': model_type,\n",
    "    'metrics': {\n",
    "        'MAE': float(results_df.loc[best_model_name, 'MAE']),\n",
    "        'RMSE': float(results_df.loc[best_model_name, 'RMSE']),\n",
    "        'R2': float(results_df.loc[best_model_name, 'R2'])\n",
    "    },\n",
    "    'feature_columns': feature_cols\n",
    "}\n",
    "\n",
    "joblib.dump(model_info, '../models/model_info.pkl')\n",
    "\n",
    "print(f\"\\n‚úÖ Model artifacts saved!\")\n",
    "print(f\"   - Best model: models/best_model.{'keras' if model_type == 'keras' else 'pkl'}\")\n",
    "print(f\"   - Feature engineer: models/feature_engineer.pkl\")\n",
    "print(f\"   - Scaler: models/scaler.pkl\")\n",
    "print(f\"   - Model info: models/model_info.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584612fa",
   "metadata": {},
   "source": [
    "## 11. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìä MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìÅ Dataset:\")\n",
    "print(f\"   - Training samples: {len(X_train):,}\")\n",
    "print(f\"   - Validation samples: {len(X_val):,}\")\n",
    "print(f\"   - Test samples: {len(X_test):,}\")\n",
    "print(f\"   - Features: {input_dim}\")\n",
    "\n",
    "print(f\"\\nü§ñ Models Trained:\")\n",
    "print(f\"   Baseline Models: {len(baseline_results)}\")\n",
    "print(f\"   Deep Learning Models: {len(dl_results)}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   MAE: {results_df.loc[best_model_name, 'MAE']:.2f} runs\")\n",
    "print(f\"   RMSE: {results_df.loc[best_model_name, 'RMSE']:.2f} runs\")\n",
    "print(f\"   R¬≤: {results_df.loc[best_model_name, 'R2']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Improvement over Linear Regression:\")\n",
    "lr_mae = baseline_results['Linear Regression']['MAE']\n",
    "best_mae = results_df.loc[best_model_name, 'MAE']\n",
    "improvement = ((lr_mae - best_mae) / lr_mae) * 100\n",
    "print(f\"   MAE improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Model training complete! Ready for deployment.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
